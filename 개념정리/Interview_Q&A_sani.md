# Interview Q

## [Deep Learning]
01. 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?
  + 딥러닝은 머신러닝에 속하는 개념으로, 많은 데이터로 뉴럴 네트워크, 역전파 알고리즘 등을 이용하여 학습하고 결과를 도출함.
  + 머신러닝은 뉴럴 네트워크가 아닌 확률 모델을 이용하며 딥러닝이 머신러닝보다 많은 수의 데이터를 필요로함.
02. 왜 갑자기 딥러닝이 부흥했을까요?
  + 빅데이터 개념이 나오면서 많은 양의 데이터를 모아 처리하게 되고, GPU의 사용으로 연산을 빠르게 처리할 수 있게 되었으며, 기존 알고리즘(back propagation, vanishing gradient solution)이 개선되며 딥러닝이 부흥
03. 마지막으로 읽은 논문은 무엇인가요? 설명해주세요
04. Cost Function과 Activation Function은 무엇인가요?
  + 비용함수는 예측값과 실제값의 오차를 계산하는 함수로, 최소 비용을 최소화하는 방향으로 파라미터를 업데이트하는데 쓰임
  + 활성함수는 입력 데이터를 다음레이어로 출력할지 결정하는 함수로, 선형 시스템을 비선형 시스템으로 바꿀 수 있음
05. Tensorflow, Keras, PyTorch, Caffe, Mxnet 중 선호하는 프레임워크와 그 이유는 무엇인가요?
06. Data Normalization은 무엇이고 왜 필요한가요?
  + 데이터를 왜곡하지 않으면서 공통스케일로 변경하여 신경망 학습을 빠르게하는 방법으로, 정규화한경우 최소비용 수렴 속도가 빠르며 로컬미니멈에서 더 빨리 빠져나올 수 있다.
07. 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)
  + sigmoid: 로지스틱 회귀, 이진분류 등에 사용되며 레이어가 깊어질수록 기울시 소실되는 vanishing gradient 문제가 있음
  +	Tanh: [-1,1]범위값에 평균이 0이기때문에 데이터를 원점으로 이동하는 효과가 있어 sigmoid보다 효율적이며 RNN, LSTM 학습에 사용
  +	ReLU: 특정값에 수렴하지 않고, 레이어가 많아져도 빠르게 학습가능하나 값이 0이하일땐 모두0이되는 dying ReLU 현상 발생 
  +	ELU, leaky ReLU: dying ReLU를 방지하는 함수로 0미만일때 leaky는 선형, ELU는 비선형적
  + softmax: 결과를 확률로 변경하는 함수로 다중 클래스 분류시 사용
  + sigmoid와 Tanh는 z값이 커질수록 기울기가 작아져 학습속도가 느림. 따라서 주로 은닉층으로는 ReLU를 많이 쓰고 sigmoid는 이진분류의 출력층에 주로 쓰이며 Tanh가 sigmoid보다 주로 좋은 성능을 보임
08. 오버피팅일 경우 어떻게 대처해야 할까요?
  + 변수 개수를 줄이고 모델을 간단하게 만드는 방법, k-fold cross validation 사용, 정규화를 사용하는 방법이 있음.
09. 하이퍼 파라미터는 무엇인가요?
  + 파라미터를 통제하는 파라미터로 여러번의 시도를 통해 적합한 하이퍼 파라미터를 찾아내야한다. 하이퍼 파라미터에는 학습률, 반복 횟수, 은닉층 갯수, 은닉유닛 갯수, 활성화 함수, 모멘텀항, 미니배치크기가 있음.
10. Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?
11. 볼츠만 머신은 무엇인가요?
12. 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?
	> 1. Non-Linearity라는 말의 의미와 그 필요성은?
	> 2. ReLU로 어떻게 곡선 함수를 근사하나?
	> 3. ReLU의 문제점은?
	> 4. Bias는 왜 있는걸까?
13. Gradient Descent에 대해서 쉽게 설명한다면?
	> 1. 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?
	> 2. GD 중에 때때로 Loss가 증가하는 이유는?
	> 3. 중학생이 이해할 수 있게 더 쉽게 설명 한다면?
	> 4. Back Propagation에 대해서 쉽게 설명 한다면?
14. Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?
	> 1. GD가 Local Minima 문제를 피하는 방법은?
	> 2. 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?
15. Training 세트와 Test 세트를 분리하는 이유는?
	> 1. Validation 세트가 따로 있는 이유는?
	> 2. Test 세트가 오염되었다는 말의 뜻은?
	> 3. Regularization이란 무엇인가?
16. Batch Normalization의 효과는?
	> 1. Dropout의 효과는?
	> 2. BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?
	> 3. GAN에서 Generator 쪽에도 BN을 적용해도 될까?
17. SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?
	> 1. SGD에서 Stochastic의 의미는?
	> 2. 미니배치를 작게 할때의 장단점은?
	> 3모멘텀의 수식을 적어 본다면?
18. 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?
	> 1. 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?
	> 2. Back Propagation은 몇줄인가?
	> 3. CNN으로 바꾼다면 얼마나 추가될까?
19. 간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데 몇시간이 필요한가?
	> 1. CNN이 아닌 MLP로 해도 잘 될까?
	> 2. 마지막 레이어 부분에 대해서 설명 한다면?
	> 3. 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?
	> 4. 만약 한글 (인쇄물) OCR을 만든다면 데이터 수집은 어떻게 할 수 있을까?
20. 딥러닝할 때 GPU를 쓰면 좋은 이유는?
	> 1. 학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는?
	> 2. GPU를 두개 다 쓰고 싶다. 방법은?
	> 33. 학습시 필요한 GPU 메모리는 어떻게 계산하는가?
21. TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?
22. 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?

---
[출처] https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/

